---
layout: page
title: "Research"
---
### PAPERS & SELECTED WIPs
Below are my past and on-going work as well as existing puzzles. Note: In computer and information science, papers published in major conference proceedings are double-anonymously peer reviewed and recognized as equivalent research contributions to journal articles.

### I. Machine Learning Research Communication
_<span style="color: #7e7e7e;">Machine Learning scientists are inextricably connected to downstream impact but it remains a challenge to trace concrete paths of their “foundational” research to harms, a distinguishing feature from ML practitioners. How should Machine Learning scientists navigate the responsibilities of producing sound and/or usable knowledge and the responsibilities for the social impact that their research outputs undergird? To this end, I have developed normative arguments and conceptual tools for holding ML scientists accountable for social impact through research communication.</span>_

{% include paper.html
  title="Dead Zone of Accountability"
  title_url="https://arxiv.org/pdf/2404.13131?"
  authors="(W/Dana Calacci and Cindy Lin)"
  venue="<em>Proceedings of the AAAI/ACM Conference on Artificial Intelligence, Ethics and Society (AIES)</em>, 2025"
  tldr="[TL;DR]: This paper introduces the concepts of the <em>claim–reality gap</em> and the <em>dead zone of accountability</em> to diagnose why social claims in ML research resist scrutiny, and proposes strategies to articulate and defend such claims as a way of creating accountability upstream, before harms manifest."
  abstract="Many Machine Learning research studies use language that describes potential social benefits or technical affordances of new methods and technologies. Such language, which we call “social claims”, can help garner substantial resources and influence for those involved in ML research and technology production. However, there exists a gap between social claims and reality (the claim–reality gap): ML methods often fail to deliver the claimed functionality or social impacts. This paper investigates the claim–reality gap and makes a normative argument for developing accountability mechanisms for it. In making the argument, we make three contributions. First, we show why the symptom—absence of social claim accountability—is problematic. Second, we coin dead zone of accountability—a lens that scholars and practitioners can use to identify opportunities for new forms of accountability. We apply this lens to the claim–reality gap and provide a diagnosis by identifying cognitive and structural resistances to accountability in the claim–reality gap. Finally, we offer a prescription—two potential collaborative research agendas that can help create the conditions for social claim accountability."
%}

{% include paper.html
  title="How Claim Replicability Can Help Bridge the Responsibility Gap"
  title_url="https://dl.acm.org/doi/10.1145/3630106.3658951"
  venue="<em>Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT)</em>, 2024"
  tldr="[TL;DR]: ML scientists should be accountable not just for replicating model performance but for making replicable claims about their research. Shifting the focus to claim replicability embeds accountability into research communication, helping bridge the responsibility gap between scientists and downstream impacts."
  abstract="Two goals—improving replicability and accountability of Machine Learning research respectively—have accrued much attention from the AI ethics and the Machine Learning community. Despite sharing the measures of improving transparency, the two goals are discussed in different registers: replicability registers with scientific reasoning whereas accountability registers with ethical reasoning. Given the existing challenge of the responsibility gap—holding Machine Learning scientists accountable for Machine Learning harms due to them being far from sites of application—this paper posits that reconceptualizing replicability can help bridge the gap. Through a shift from model performance replicability to claim replicability, Machine Learning scientists can be held accountable for producing non-replicable claims that are prone to eliciting harm due to misuse and misinterpretation. In this paper, I make the following contributions. First, I define and distinguish two forms of replicability for ML research that can aid constructive conversations around replicability. Second, I formulate an argument for claim-replicability’s advantage over model performance replicability in justifying assigning accountability to Machine Learning scientists for producing non-replicable claims and show how it enacts a sense of responsibility that is actionable. In addition, I characterize the implementation of claim replicability as more of a social project than a technical one by discussing its competing epistemological principles and practical implications on Circulating Reference, Interpretative Labor, and research communication."
%}

### II. Technology Capabilities Communication
_<span style="color: #7e7e7e;">Machine Learning in its various forms (from early rule-based algorithms to now large-scale architectures) have galvanized the development of AI tools for a wide variety of application domains. However, premature adoptions of these tools in have bred cultural harms (such as hype) and concrete harms (such as inequity and exploitation). To this end, I study how these harms arise from how different stakeholder groups communicate about certain technologies’ capabilities and how to mitigate these harms through responsible communication.</span>_

### III. Enforcing Responsible Communication
_<span style="color: #7e7e7e;">A core issue to the scholarship on socially responsible computing is translating conceptual understandings & tools into practical solutions. A key strand of my future research is translating my theorization into practical tools, policies, and legal frameworks. To this end, I have several work-in-progress.</span>_
